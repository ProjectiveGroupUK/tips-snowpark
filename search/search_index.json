{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to TiPS (Snowpark)","text":""},{"location":"#what-is-tips","title":"What is TiPS?","text":"<p>TiPS is a simple data transformation and data quality framework built for Snowflake.</p> <p>With TiPS, building data pipelines is as easy as writing your transformation business logic in a database view which acts as data source, have a target table defined where this transformed data would land and add a metadata record in TiPS metadata table and that's it. You don't need to write or execute any DML statements for movement of this data. TiPS at runtime would dynamically generate and execute all DML statements relevant to the steps of data pipeline.</p> <p>Along with INSERT/APPEND/MERGE/DELETE/TRUNCATE, TiPS also comes pre-built with command types like:</p> <ol> <li>PUBLISH_SCD2 - supports pushing data to SCD2 (slow changing type2) dimension, where for changed records, a new record is inserted, and existing record is updated with applicable effective start and end dates</li> <li>COPY_INTO_FILE - supports extracting data from snowflake database to a file in a stage. Stage can be external, internal or a named stage.</li> <li>COPY_INTO_TABLE - supports loading data from a staged file into snowflake database table. Stage can be external, internal or a named stage.</li> </ol> <p>Details about all possible command types in TiPS are listed on reference guide page</p> <p>The ideology behind TiPS was to create a framework that an experienced database professional, already adept with SQL, could easily deliver data pipelines with virtually zero learning curve.</p> <p>A data pipeline in TiPS is made up multiple steps that can be chained together, with each step performing its own operation to move data from source to target or checking data quality. Steps within a data pipeline can perform one of two things:</p> <ul> <li>A movement of data from a source to a target. In most cases the sources are database views encapsulating transformation logic in the desired form, while the targets are database tables.</li> <li>A data quality check to make sure data being moved from source to target conforms to the desired form, before getting consumed by the data consumer and thus providing inconsistent results.</li> </ul> <p></p> <p>TiPS was built with security in mind. The database credentials used to execute a pipeline do not require read/write access to the underlying data, in this regard TiPS differs from other data transformation tools. We believe that data pipelines should be idempotent, this being the case if pipeline execution credentials were ever leaked the worst-case scenario is that a pipeline could be re-executed (compute costs would increase but data integrity would not be compromised).</p>"},{"location":"#what-tips-is-not","title":"What TiPS is not?","text":"<p>TiPS is not a scheduler/orchestrator tool: TiPS doesn't have any scheduling or orchestration capabilities built in. Orchestrating or scheduling for execution of data pipelines on a regular interval, can be done through other tools like Airflow, Control-M or Unix Cron for that matter.</p> <p><p>TiPS is not a Data Ingestion tool: TiPS is a transformation framework, and is not placed to be a replacement for data ingestion tools like Fivetran, Matillion etc. With TiPS, usually the starting source of data for the data pipeline is, either data already landed into Snowflake tables from source, or from files stored in a Snowflake accessible stage (external or internal, E.g. S3 on AWS).</p>"},{"location":"#how-does-tips-work","title":"How does TiPS work?","text":"<p>TiPS is a simple to use Metadata driven transformation framework. All the metadata is stored in database tables in Snowflake, which can easily be interrogated using normal SQL commands.</p> <p>All TiPS objects are first class database objects</p> <p>When run in Snowpark through stored procedure, TiPS provides an extra security feature where the executing user of the stored procedure doesn't need to have direct read/write privileges on the underlying table/data. User calling the stored procedure only needs privileges to execute the stored procedure.</p>"},{"location":"#tips-metadata-tables","title":"TiPS Metadata Tables:","text":"<ul> <li>PROCESS - Holds information about Data Pipeline e.g., Name and Description of Data Pipeline.</li> <li>PROCESS_CMD - This table holds information about individual steps within a data pipeline.</li> <li>PROCESS_LOG - This table is populated with data pipeline execution logs when data pipelines are run through TiPS</li> <li>PROCESS_DQ_TEST - This table is shipped with some preconfigured DQ tests. New tests can be configured by the users themselves into this table.</li> <li>PROCESS_CMD_TGT_DQ_TEST - This table is configured with Linking DQ Tests to the Target (table).</li> <li>PROCESS_DQ_LOG - This table is populated with data quality test execution logs when data pipelines are run through TiPS. Data in this table is tied up to <code>PROCESS_LOG</code> table through  <code>process_log_id</code> column.</li> </ul>"},{"location":"#licencing","title":"Licencing","text":"<p>TiPS is licenced under the MIT Open Source licence giving you flexibility to use it as you wish.<p>Any feedbacks and suggestions for improvements are always welcome. Kindly add your feedbacks/suggestions using GitHub Discussions </p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Great, now that you have made your mind to give TiPS a try, let's get you started. We are pretty confident that you will find it worth to give it a try. </p> <p>Python (3.8 or later) is required to run TiPS using the Snowpark Client API or to test the code before uploading it to Snowflake. See the Python's official website for installation instructions.</p>"},{"location":"getting-started/#tips-setup","title":"TiPS Setup","text":"<p>Presumably you are currently browsing through the repository on GitHub or perhaps you have already downloaded the repository to your PC. If you haven't yet downloaded the repository, let's get it downloaded first.</p>"},{"location":"getting-started/#downloading-repository","title":"Downloading Repository","text":"<p>TiPS can be downloaded from GitHub repository, either by cloning the repository or downloading as zip file. When on the repository page, click on the \"&lt;&gt; Code\", as shown in yellow highlighting, in screenshot below:</p> <p>.</p> <p>From here you can either clone the repository to your PC or download the source code as Zip file. </p> <p>If you have downloaded the zip file, unzip it into a local folder. Folder name can be as per your choice, however if you create it as <code>c:\\GitHub\\tips-snowpark</code> (on Windows) or <code>/GitHub/tips-snowpark</code> (on Unix based OS, including Mac), it will make it much easier for you to copy/paste the commands from the instructions.</p>"},{"location":"getting-started/#create-python-virtual-environment","title":"Create Python Virtual Environment","text":"<p>Now that you have got the code on your PC in your desired folder location, open the code in editor of your choice (e.g., VSCode or PyCharm).</p> <p>Open the terminal windows from your code editor (which should ideally open the terminal in repository folder)<p>OR</p><p>if you prefer, open command prompt/PowerShell (Windows) or terminal window (Linux/Mac) and navigate to repository folder.</p></p> <p>For example:</p> <ul> <li>Windows - <code>cd C:\\GitHub\\tips-snowpark\\</code></li> <li>Linux/Mac - <code>cd /GitHub/tips-snowpark</code></li> <li>Bash Terminal - <code>cd /c/GitHub/tips-snowpark/</code></li> </ul> <p>It is preferable to use Bash Terminal on Windows PC, as command examples in the instructions given below are from bash terminal. On Windows PC with Git installed, Git Bash should be present.</p> <p>Once in the terminal, run the following command to setup python virtual environment:</p> <p><code>$ python -m venv venv</code> (this would create a folder named venv in your current location)</p> <p>Next run command: <code>$ source venv/Scripts/active</code> (to activate virtual environment)</p> <p>Next run command: <code>$ pip install -r requirements.txt</code> (this would install all required dependencies to virtual environment)</p> <p>Now your environment is all set, so we can mode to next step.</p>"},{"location":"getting-started/#setup-database-objects","title":"Setup Database Objects","text":"<p>There is a folder named <code>metadata_tables_ddl</code>, which contains SQL scripts for database objects that are part of TiPS. Execute these scripts on Snowflake, in the order they are listed. You can run these in any schema of your choice, but if there is no preference, suggested schema is <code>TIPS_MD_SCHEMA</code> (create schema or use existing one, as deemed fit)</p> <ol> <li><code>process.sql</code>: This is DDL script for <code>PROCESS</code> table.</li> <li><code>process_cmd.sql</code>: This is DDL script for <code>PROCESS_CMD</code> table.</li> <li><code>process_log_seq.sql</code>: This script creates a sequence, needed for <code>PROCESS_LOG</code> table.</li> <li><code>process_log.sql</code>: This is DDL script for <code>PROCESS_LOG</code> table.</li> <li><code>process_dq_test.sql</code>: This is DDL script for <code>PROCESS_DQ_TEST</code> table. This script also populates standard DQ Test data shipped with the code.</li> <li><code>process_cmd_tgt_dq_test.sql</code>: This is DDL script for <code>PROCESS_CMD_TGT_DQ_TEST</code> table.</li> <li><code>process_dq_log.sql</code>: This is DDL script for <code>PROCESS_DQ_LOG</code> table.</li> <li><code>vw_process_log.sql</code> [optional]: This is script for a view to be built on PROCESS_LOG table. It flattens out the JSON Log into tabular rows for easier interpretation from within the database.</li> <li><code>tips_internal_stage.sql</code>: This script creates an internal named stage, which would need to upload our code base, which is referenced by stored procedure.</li> </ol> <p>Also, inside <code>stored_procedure_stub</code> folder, there are couple of scripts available. These are for stored procedures used by TiPS. Please compile <code>create_temporary_table.sql</code> script. This stored procured is needed even when we run TiPS from command line (using Snowpark Client API) </p> <p>Now that we have created Database objects required by TiPS, we are ready to move to the next step to setup our first sample pipeline to test run TiPS.</p>"},{"location":"getting-started/#setup-sample-data-pipeline-optional","title":"Setup Sample Data Pipeline [optional]","text":"<p>To see TiPS in action after the setup, you can follow the steps below to setup a trial data pipeline without having to write any code yourself.</p> <p>For setting up a trial data pipeline, we have supplied with SQL scripts that you can run on your Snowflake account. These scripts are in sample_pipeline folder under test folder, within this repository.</p>"},{"location":"getting-started/#schema-scripts","title":"Schema Script(s)","text":"<p>There are couple of scripts in \"test -&gt; sample_pipeline -&gt; schema\" folder. Please run these scripts to create two schemas where would setup tables and views for our pipeline. Please run both <code>tips_test_dimension.sql</code> and <code>tips_test_transform.sql</code> scripts.</p>"},{"location":"getting-started/#sequence-scripts","title":"Sequence Script(s)","text":"<p>Next, please run the scripts available in \"test -&gt; sample_pipeline -&gt; sequence\" folder.</p>"},{"location":"getting-started/#table-ddl-scripts","title":"Table DDL Script(s)","text":"<p>Next, please run the scripts available in \"test -&gt; sample_pipeline -&gt; table\" folder.</p>"},{"location":"getting-started/#view-scripts","title":"View Script(s)","text":"<p>Next, please run the scripts available in \"test -&gt; sample_pipeline -&gt; view\" folder.</p>"},{"location":"getting-started/#metadata-scripts","title":"Metadata Script(s)","text":"<p>Finally, for setup of data pipeline, run the script available in \"test -&gt; sample_pipeline -&gt; metadata\" folder. This script sets up TiPS metadata needed for execution of data pipeline.</p>"},{"location":"getting-started/#execute-data-pipeline-using-snowpark-client-api","title":"Execute Data Pipeline (using Snowpark Client API)","text":"<p>Now that you have setup a trial data pipeline using table/views and metadata, it is now a good time to test whether TiPS is working as expected.</p> <p>To do that please follow the steps below:</p> <p>First, we need to setup a file with your database credentials that TiPS would use to connect to your Snowflake instance. To do that, inside <code>test</code> folder, create a file named <code>.env</code> (There is no filename, just the extension. This file is already configured to be ignored from getting pushed to Git repository). Once you have the file in <code>test</code> folder, open it in any of your favourite text editor and add your database credentials as below (change the values appropriate to your account between <code>&lt;&lt;&gt;&gt;</code>):</p> <pre><code>SF_ACCOUNT=&lt;&lt;Snowflake Account ID&gt;&gt;\nSF_USER=&lt;&lt;Snowflake User ID&gt;&gt;\nSF_PASSWORD=&lt;&lt;Your Snowflake Password&gt;&gt;\nSF_ROLE=&lt;&lt;Snowflake Role to use&gt;&gt; \nSF_WAREHOUSE=&lt;&lt;Snowflake Warehouse to use&gt;&gt;\nSF_DATABASE=&lt;&lt;Snowflake Database to connect to&gt;&gt;\nSF_SCHEMA=&lt;&lt;Snowflake schema where TiPS metadata information has been setup&gt;&gt;\n</code></pre> <p>For executing TiPS from command line using Snowpark Client API, we will run it with <code>run_process_log.py</code> which is available inside test folder. However, before we run that, we would need to set an environment variable on the terminal, for relative paths in imports to work properly. Please use the command below as an example and tweak as necessary according to your folder structure setup and/or OS requirement (below is the example for being run on a Bash terminal) you would need to set PYTHONPATH environment variable, with path that of your github folder </p> <p>E.g.</p> <pre><code>export PYTHONPATH=/c/GitHub/tips-snowpark\n</code></pre> <p>Now navigate to test folder, if you are not already in it.</p> <pre><code>cd /c/GitHub/tips-snowpark/test\n</code></pre> <p>And then to execute the trial pipeline that we have setup in previous steps, you can execute one of the commands below:</p> <p>Serial Execution (Step executed serially following topolical sorting):</p> <pre><code>python run_process_local.py -p TIPS_TEST_PIPELINE -v \"{'COBID':'20230101', 'MARKET_SEGMENT':'FURNITURE'}\" -e Y\n</code></pre> <p>Parallel Execution (Step executed parallely utilising Snowflake Tasks):</p> <pre><code>python run_process_with_tasks_local.py -p TIPS_TEST_PIPELINE -v \"{'COBID':'20230101', 'MARKET_SEGMENT':'FURNITURE'}\" -e Y\n</code></pre> <p>With parameters:</p> <ul> <li>-p -&gt; We are passing in Data Pipeline Name that we have setup in previous steps.</li> <li>-v -&gt; Here we are passing in the bind variable values in JSON format, that are used by our pipeline. This is an optional parameter and is only required when bind variables are used in the pipeline.</li> <li>-e -&gt; Here we pass in Y/N flag, for indicating whether we want to execute the SQLs generated by TiPS or want to generate the SQLs and outputted for debugging purpose. If Y is passed, then generated SQLs are executed on the database. If N is passed, then generated SQLs are only outputted to the log and are not executed in the database.</li> </ul> <p>Once the above command is executed and if everything has been setup correctly, you should start seeing the log messages on terminal window displaying how execution of pipeline is progressing. You should also notice a <code>log</code> folder created inside <code>test</code> folder, where log files are generated.</p> <p>Please Note: Parallel execution relies on parent_process_cmd_id column setting in PROCESS_CMD table. Further details about this column can be checked in Reference Guide"},{"location":"getting-started/#execute-data-pipeline-inside-snowflake-with-stored-procedure","title":"Execute Data Pipeline (inside Snowflake with Stored Procedure)","text":"<p>So, if you have reached this step, presumably everything with setting things up, has worked as expected. This step is needed, if you want to run the data pipelines from inside the database, executing via stored procedure. The advantage of running it with Stored procedure are:</p> <ol> <li>Stored procedure runs with owner privileges. So, the user executing it, doesn't need read/write privileges on underlying database objects. Only execution privilege is required on the stored procedure.</li> <li>Everything runs inside the database, so data doesn't leave the data platform.</li> <li>Once the setup is done, other users/tools can execute data pipelines without needing Python or other setup that we needed to do in steps above.</li> </ol> <p>Before we compile the stored procedure, we would need to upload TiPS core code to Snowflake internal stage, that we created in step above.</p>"},{"location":"getting-started/#package-tips-code-as-a-zip-file-and-upload-to-a-stage","title":"Package TiPS code as a zip file and upload to a stage","text":"<p>There is a \"tips\" folder inside the repository. Bundle this folder with its content to a zip file (preferably named as <code>tips.zip</code>). If you are on windows, you can easily do that from file explorer, by navigating to repository folder and then right click on tips folder and select \"compress to zip file\" option (or \"send to compressed file\" on previous versions of windows). This should produce <code>tips.zip</code> file inside your repository folder.</p>"},{"location":"getting-started/#compile-run_process-stored-procedure","title":"Compile RUN_PROCESS stored procedure","text":"<p>RUN_PROCESS stored procedure is used to execute data pipelines in serial mode. To create this stored procedure, please execute the script <code>run_process.sql</code>, which is available inside stored_procedure_stub folder.</p>"},{"location":"getting-started/#compile-run_process_with_tasks-stored-procedure","title":"Compile RUN_PROCESS_WITH_TASKS stored procedure","text":"<p>RUN_PROCESS_WITH_TASKS stored procedure is used to execute data pipelines in parallel mode. To create this stored procedure, please execute the script <code>run_process_with_tasks.sql</code>, which is also available inside stored_procedure_stub folder.</p>"},{"location":"getting-started/#execute-data-pipeline","title":"Execute Data Pipeline","text":"<p>Now that stored procedures are compiled in the database, you can execute data pipelines with TiPS directly inside from Snowflake like any other stored procedure. For example, to run the sample pipeline, you can run one of the following commands from within Snowflake IDE:</p> <p>Serial Execution (Step executed serially following topolical sorting):</p> <pre><code>call run_process(process_name=&gt;'TIPS_TEST_PIPELINE', vars=&gt;'{\"COBID\":\"20230101\", \"MARKET_SEGMENT\":\"FURNITURE\"}', execute_flag=&gt;'Y')\n</code></pre> <p>Parallel Execution (Step executed parallely utilising Snowflake Tasks):</p> <pre><code>call run_process_with_tasks(process_name=&gt;'TIPS_TEST_PIPELINE', vars=&gt;'{\"COBID\":\"20230101\", \"MARKET_SEGMENT\":\"FURNITURE\"}', execute_flag=&gt;'Y')\n</code></pre> <p>Please Note: Parallel execution relies on parent_process_cmd_id column setting in PROCESS_CMD table. Further details about this column can be checked in Reference Guide <p>In both the commands above, parameter values are passed in named-parameter way, but you can just pass in values in positional way, without explicitly specifying parameter name. Also, <code>var</code> parameter value is needed in JSON format where bind variables are used in the pipeline. If bind variables are not used, just pass in <code>NULL</code> instead.</p> <p>All Done! You are now set to start using TiPS in its full swing. Please do checkout TiPS Conventions and Reference Guide for further useful information.</p>"},{"location":"reference/","title":"Reference Guide","text":""},{"location":"reference/#metadata-tables","title":"Metadata Tables","text":""},{"location":"reference/#process","title":"PROCESS","text":"<p>This is the table where you define information about data pipeline e.g., Name of Pipeline and other information.</p> Column Name Description PROCESS_ID Sequentially generated ID.<p> This is defined as Auto increment on table, so doesn't need to be included in DMLs</p> PROCESS_NAME Enter Name of Data Pipeline.<p> This value is passed as parameters when TiPS is executed. No whitespaces to be used in process name and preferably use Uppercase</p> PROCESS_DESCRIPTION Description about Data Pipeline.<p> This is optional field, but good to have proper description about the pipeline for others to easily understand</p> ACTIVE Active (Y) / Inactive (N) flag.<p> When set to N (inactive), data pipeline can be disabled and TiPS will not execute it</p>"},{"location":"reference/#process_cmd","title":"PROCESS_CMD","text":"<p>This is the table where you populate information about each step in a data pipeline. There are several columns in this table, some of which are specific to command types. Out of those some are mandatory, and some are optional. Further details about command types are documented below </p> Column Name Description PROCESS_ID Foreign Key to PROCESS_ID in PROCESS table<p>This needs to match ID of data pipeline defined in PROCESS table</p> PROCESS_CMD_ID Manually assigned sequential ID for Data Pipeline Step<p>When TiPS is executed, steps of a data pipeline run serially in sequential order. Order of the step is identified using this ID. It is advisable to leave gaps between the IDs when setting up the data pipeline initially, so that if there is a need to add an interim step later, it can be inserted in between without needing to reassign all IDs. E.g., 10,20,30,40... or 100,200,300,400... can be assigned initially which leave sufficient gap for steps to be inserted in between, if need be, later. Also in a big team, where multiple data engineers might be working on same data pipeline in same development iteration (sprint), mechanism needs to be in place so that data engineers don't end up using overlapping IDs</p> CMD_TYPE This describes the operation type. Further details are given below <p>E.g., APPEND / REFRESH / DELETE / TRUNCATE etc. All CAPS without spaces</p> CMD_SRC Specify the name of source of data here. <p>This is usually a data table or a view that encapsulates the transformation business logic. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_TGT Specify the name of target destination of data here. <p>This is usually a table. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_WHERE This is where you can specify a filter clause that gets added to source as a WHERE clause at run time. <p>WHERE keyword should not be included and numbered bind variables can be used for which actual bind replacements are mentioned in CMD_BINDS E.g.<p>\"C_MKTSEGMENT = :2 AND COBID = :1\"</p>In the above example values for bind variables are passed at run time, and derivation of value for bind variable according to the sequence is derived from CMD_BINDS CMD_BINDS If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpreted as keys from the variable values JSON object passed in at run-time</p> REFRESH_TYPE This is only applicable for REFRESH command type. Acceptable values are DI/TI/OI <p>DI (Delete Insert) - Before inserting the data in target, a delete command is run (where optionally filter clause can be added through CMD_WHERE).</p><p>TI (Truncate Insert) - Before inserting the data in target, truncate command on target is run.</p><p>OI (Overwrite Insert) - Before inserting the data in target, any existing data is removed from target. This works similar to truncate, with the caveat that TRUNCATE is a DDL command invoking a commit to the transaction where OVERWRITE doesn't commits the transaction immediately after delete, thus tables is rolled back to previous state if INSERT DML throws an error.</p> BUSINESS_KEY This is only applicable for PUBLISH_SCD2_DIM command type. It is column(s) delimited by Pipe \"|\" symbol that defines a business key (also referred as natural key) for a dimension table. <p>For a slowly changing dimension, this is combination of key columns that uniquely identifies a row in the dimension (not a surrogate key), excluding record effective dates and/or current record flag</p> MERGE_ON_FIELDS This is only applicable for MERGE command type. Here you specify columns that are to be used in generated MERGE SQL in the ON join clause. Multiple fields delimited by Pipe \"|\" symbol GENERATE_MERGE_MATCHED_CLAUSE This is only applicable for MERGE command type. Here you specify whether ON MATCHED CLAUSE is to be generated in generated MERGE DML. Accepted values are Y/N. When Y is selected, ON MATCHED CLAUSE is generated which runs an UPDATE operation GENERATE_MERGE_NON_MATCHED_CLAUSE This is only applicable for MERGE command type. Here you specify whether ON NOT MATCHED CLAUSE is to be generated in generated MERGE DML. Accepted values are Y/N. When Y is selected, ON NOT MATCHED CLAUSE is generated which runs an INSERT operation ADDITIONAL_FIELDS This is where you specify any additional columns/fields to be added to generated SELECT clause from source, which is not available in source. <p>E.g.</p><p>TO_NUMBER(:1) COBID</p><p>would add a column to generated SELECT statement, where it is transforming bind variable value passed in at run time, and aliased as COBID, which would then become part of INSERT/MERGE statement</p> TEMP_TABLE Acceptable values - Y/N/NULL <p>When set to Y, this would trigger creating a temporary table with the same name as target in the same schema as target before the operation of the step is run.</p><p>This feature utilises special functionality that has been introduced in Snowflake, that you can have permanent (or transient) table and a temporary table with the same name and temporary table is then given priority in the current running session.</p><p>In TiPS we utilise this functionality where a data pipeline can be run concurrently withing multiple sessions with its own bind variables and dataset are consistently transformed and published at session level CMD_PIVOT_BY This is only applicable for COPY_INTO_FILE command type. If you are looking to apply a PIVOT in the SQL query in the COPY INTO statement, then this can be used. This field will dictate which column's values are to be pivoted e.g., REPORTING_CCY CMD_PIVOT_FIELD This is only applicable for COPY_INTO_FILE command type. If you are looking to apply a PIVOT in the SQL query in the COPY INTO statement, then this can be used. This field will be the aggregation of the field for the values which are included in the pivot e.g., SUM(REPORTING_CURRENCY) ACTIVE Active (Y) / Inactive (N) flag.<p> When set to N (inactive), data pipeline step can be disabled and TiPS will skip that step while execute the pipeline FILE_FORMAT_NAME This option is applicable to COPY_INTO_FILE and COPY_INTO_TABLE command types. <p>If a file format has been defined in the database, that can be used. Please include schema name with file format name e.g. [SCHEMA NAME].[FILE FORMAT NAME] and all in CAPS please</p> COPY_INTO_FILE_PARITITION_BY This option is applicable to COPY_INTO_FILE command type. This adds PARTITION BY clause in generated COPY INTO FILE command. COPY_INTO_FILE_PARITITION_BY field needs to be an SQL expression that outputs a string. The dataset specified by CMD_SRC will then be split into individual files based on the output of the expression. A directory will be created in the stage specified by CMD_TGT which will be named the same as the partition clause. The data will then be output into this location in the stage. COPY_AUTO_MAPPING Only applicable for COPY_INTO_TABLE command type with CSV source files. <p>Acceptable values - Y/N/NULL</p> <p>When set to 'Y', only fields that exist in both the source and target will be loaded. Source file must have a header. This option can be used to reorder fields from source file to target table or omit fields from the source.</p> COPY_INTO_FORCE Only applicable for COPY_INTO_TABLE command type. <p>Acceptable values - Y/N/NULL</p><p>When set to 'Y', this option will include the FORCE = TRUE option on COPY INTO commands. Data is loaded regardless of whether source file is unchanged or data already exists in target table. Can produce duplicate data in target table </p> PARENT_PROCESS_CMD_ID This is populated with PROCESS_CMD_ID of preceeding step. Where current step has multiple predecessors, pipe delimited PROCESS_CMD_ID should be entered. For steps with no preceding steps, \"NONE\" should be used as a DEFAULT value. WAREHOUSE_SIZE If a step requires a different warehouse size to run instead of the default one used to execute the process, then the warehouse size (t-shirt sizes) can be specified here. Expected value are NULL, \"XS\", \"S\", \"M\", \"L\", \"XL\", \"2XL\", \"3XL\", \"4XL\", \"5XL\", \"6XL\". At run time, this value is replaced with part of string following the last underscore in default warehouse name (used to execute the process). <p>This setting is only applicable when running process in parallel mode.</p>"},{"location":"reference/#process_log","title":"PROCESS_LOG","text":"<p>This table holds logging information about each run of TiPS. This table is populated automatically at the end of execution of TiPS</p> Column Name Description PROCESS_LOG_ID Sequentially generated ID PROCESS_NAME Name of Data Pipeline that has been executed. PROCESS_LOG_CREATED_AT Timestamp at which log records have been inserted to the table PROCESS_START_TIME Timestamp of start of execution of Data Pipeline PROCESS_END_TIME Timestamp of completion of execution of Data Pipeline PROCESS_ELAPSED_TIME_IN_SECONDS Total time (in seconds) taken in execution of Data Pipeline EXECUTE_FLAG Whether Data Pipeline was invoked with Execution Status. When EXECUTE_FLAG is passed as N, SQL statements are only generated and logged but not executed in the database STATUS Status of Data Pipeline Execution ERROR_MESSAGE If any steps errored, top level error message is populated here LOG_JSON Complete Log information of Data Pipeline in JSON format. View <code>VW_PROCESS_LOG</code> displays flattened information of this column RUN_ID Internal ID generated at process level. When process is execute in parallel mode, each step creates a record in PROCESS_LOG with a different PROCESS_LOG_ID, but maintaining the same RUN_ID"},{"location":"reference/#process_dq_test","title":"PROCESS_DQ_TEST","text":"<p>This table is populated with data relating to Data Quality Tests. This table is shipped with some standard DQ Test definitions.</p> Column Name Description PROCESS_DQ_TEST_ID Sequentially generated ID PROCESS_DQ_TEST_NAME Uniquely identifiable Name for Data Quality Test PROCESS_DQ_TEST_DESCRIPTION Descriptive information about Data Quality Test PROCESS_DQ_TEST_QUERY_TEMPLATE Query template to be used when running Data Quality Test. Identifiers within curly braces <code>{}</code> are replaced with actual values at run time. Bind variables can also be specified inside curly braces, e.g. <code>{:1}</code>, which is then replaced by QUERY_BINDS in PROCESS_CMD_TGT_DQ_TEST at runtime PROCESS_DQ_TEST_ERROR_MESSAGE Error Message to display when Test fails ACTIVE TRUE/FALSE<p> When FALSE, data quality test would not run"},{"location":"reference/#process_cmd_tgt_dq_test","title":"PROCESS_CMD_TGT_DQ_TEST","text":"<p>This is the table that you populate with the information to enforce a predefined Data Quality test to a target (table) and additionally an attribute (column) </p> Column Name Description PROCESS_CMD_TGT_DQ_TEST_ID Sequentially generated ID TGT_NAME Specify the name of target on which Data Quality Test is to be run. <p>This is usually a table. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please.</p><p>This should match target name defined on <code>PROCESS_CMD</code> table ATTRIBUTE_NAME Enter column name on which Data Quality Test is to be run ACCEPTED_VALUES For \"Accepted Values\" test, this should contain comma separated values that are acceptable in the target.<p>E.g.</p><p><code>'AFRICA','MIDDLE EAST','EUROPE','AMERICA'</code></p> ERROR_AND_ABORT TRUE/FALSE, indicating whether the process (data pipeline) should produce error and abort execution when this data quality test fails. When FALSE, process would just log warning and process would continue ACTIVE TRUE/FALSE<p> When FALSE, data quality test would not run QUERY_BINDS Here you can enter any arbitriary bind values that you want to be used in query template. Multiple values can be entered delimited by pipe. Bind variable defined in PROCESS_DQ_TEST_QUERY_TEMPLATE in PROCESS_DQ_TEST are replaced by these values in the sequence of order (starting from :1) at runtime"},{"location":"reference/#process_dq_log","title":"PROCESS_DQ_LOG","text":"<p>This table holds logging information about each Data Quality test expected withing a data pipeline when TiPS is run. This log is also associated to data in <code>PROCESS_LOG</code> table.</p> Column Name Description PROCESS_DQ_LOG_ID Sequentially generated ID PROCESS_LOG_ID ID linking to record in <code>PROCESS_LOG</code> table TGT_NAME Name of target on which data quality test was executed ATTRIBUTE_NAME Attribute/Column on which data quality test was executed DQ_TEST_NAME Data Quality Test Name DQ_TEST_QUERY Transposed Query executed for the test DQ_TEST_RESULT Array of values causing failure. For successful test, it should be an empty array <code>[]</code> START_TIME Timestamp of start of execution of DQ Test Query END_TIME Timestamp of completion of execution of DQ Test Query ELAPSED_TIME_IN_SECONDS Total time (in seconds) taken in execution of DQ Test Query STATUS Status [PASSED or ERROR or WARNING] of DQ Test STATUS_MESSAGE Warning or Error Message returned RUN_ID Internal ID generated at process level. When process is execute in parallel mode, each step creates a record in PROCESS_LOG with a different PROCESS_LOG_ID, but maintaining the same RUN_ID"},{"location":"reference/#command-types","title":"Command Types","text":""},{"location":"reference/#append","title":"APPEND","text":"<p>This effectively generates an \"INSERT INTO [target table] ([columns]) SELECT [columns] FROM [source] additionally WHERE\", if applicable</p> <p>Following are the fields applicable for APPEND command type:</p> Field Name Mandatory? Description CMD_SRC Yes Specify the name of source of data here. <p>This is usually a data table or a view that encapsulates the transformation business logic. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_TGT Yes Specify the name of target destination of data here. <p>This is usually a table. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_WHERE No This is where you can specify a filter clause that gets added to source as a WHERE clause at run time. <p>WHERE keyword should not be included and numbered bind variables can be used for which actual bind replacements are mentioned in CMD_BINDS E.g.<p>\"C_MKTSEGMENT = :2 AND COBID = :1\"</p>In the above example values for bind variables are passed at run time, and derivation of value for bind variable according to the sequence is derived from CMD_BINDS CMD_BINDS No If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpreted as keys from the variable values JSON object passed in at run-time</p> ADDITIONAL_FIELDS No This is where you specify any additional columns/fields to be added to generated SELECT clause from source, which is not available in source. <p>E.g.</p><p>TO_NUMBER(:1) COBID</p><p>would add a column to generated SELECT statement, where it is transforming bind variable value passed in at run time, and aliased as COBID, which would then become part of INSERT/MERGE statement</p> TEMP_TABLE No Acceptable values - Y/N/NULL <p>When set to Y, this would trigger creating a temporary table with the same name as target in the same schema as target before the operation of the step is run.</p><p>This feature utilises special functionality that has been introduced in Snowflake, that you can have permanent (or transient) table and a temporary table with the same name and temporary table is then given priority in the current running session.</p><p>In TiPS we utilise this functionality where a data pipeline can be run concurrently withing multiple sessions with its own bind variables and dataset are consistently transformed and published at session level</p>"},{"location":"reference/#copy_into_file","title":"COPY_INTO_FILE","text":"<p>This command type is for outputting data from a table/view into a file into an internal user stage or an internal named stage or an external stage.</p> <p>Following are the fields applicable for COPY_INTO_FILE command type:</p> Field Name Mandatory? Description CMD_SRC Yes Specify the name of source of data here. <p>This is usually a data table or a view that would provide data in desired form. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_TGT Yes This is the file location for outputting data from source to this file. If the PARTITION_CLAUSE is not being used, then the CMD_TGT should be the exact path and filename that should be created. <p>If the PARTTION_CLAUSE is being used, then this field should only contain a stage and a path.</p><p>For example, if the file should go into a user stage in the XYZ directory it should be @~/XYZ.</p><p>In both cases, it is permissible to use any BIND variables as part of the name i.e., @~/:1/XYZ would create a directory based on the first bind variable followed by XYZ.</p> CMD_WHERE No This is where you can specify a filter clause that gets added to source as a WHERE clause at run time. <p>WHERE keyword should not be included and numbered bind variables can be used for which actual bind replacements are mentioned in CMD_BINDS E.g.<p>\"C_MKTSEGMENT = :2 AND COBID = :1\"</p>In the above example values for bind variables are passed at run time, and derivation of value for bind variable according to the sequence is derived from CMD_BINDS CMD_BINDS No If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpreted as keys from the variable values JSON object passed in at run-time</p> CMD_PIVOT_BY No If you are looking to apply a PIVOT in the SQL query in the COPY INTO statement, then this can be used. This field will dictate which column's values are to be pivoted e.g., REPORTING_CCY CMD_PIVOT_FIELD No If you are looking to apply a PIVOT in the SQL query in the COPY INTO statement, then this can be used. This field will be the aggregation of the field for the values which are included in the pivot e.g., SUM(REPORTING_CURRENCY) FILE_FORMAT_NAME No If a file format has been defined in the database with all applicable configurations, that this field can be used. Please include schema name with file format name e.g. [SCHEMA NAME].[FILE FORMAT NAME] and all in CAPS please<p>If this field is omitted, then File Type \"CSV\" and compression \"GZIP\" is used by default COPY_INTO_FILE_PARITITION_BY No This field is to be populated when you want to apply a PARTITION BY clause in generated COPY INTO FILE command. COPY_INTO_FILE_PARITITION_BY field needs to be an SQL expression that outputs a string. The dataset specified by CMD_SRC will then be split into individual files based on the output of the expression. A directory will be created in the stage specified by CMD_TGT which will be named the same as the partition clause.  The data will then be output into this location in the stage."},{"location":"reference/#copy_into_table","title":"COPY_INTO_TABLE","text":"<p>This command type is for loading data from a staged file to the database table. Stage file can be stored in an internal user stage or an internal named stage or an external stage.</p> <p>Following are the fields applicable for COPY_INTO_TABLE command type:</p> Field Name Mandatory? Description CMD_SRC Yes This is the file location with data to be loaded. CMD_SRC should be the exact path and filename that should be loaded. <p>It is permissible to use any BIND variables as part of the name i.e., @~/:1/XYZ/ABC.csv would create a directory based on the first bind variable followed by XYZ.</p><p>E.g.,</p><p>@tips/EXTRACTS/:1/PUBLISH_CUSTOMER/CUSTOMER.csv</p><p>When using File Format, extension can be omitted</p> CMD_TGT Yes This is the name of table into which data from file is to be loaded<p>Please include schema name along with table name, and all in CAPS</p> CMD_BINDS No If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpreted as keys from the variable values JSON object passed in at run-time</p> FILE_FORMAT_NAME No If a file format has been defined in the database with all applicable configurations, that this field can be used. Please include schema name with file format name e.g. [SCHEMA NAME].[FILE FORMAT NAME] and all in CAPS please<p>If this field is omitted, then File Type \"CSV\" and compression \"GZIP\" is used by default COPY_AUTO_MAPPING No <p>Acceptable values - Y/N/NULL</p> <p>When set to 'Y', only fields that exist in both the source file and target table will be loaded.  This option can be used to reorder fields from source file to target table or ommit fields from the source.</p><p> **NOTE: Source file must be a CSV with a header. ** COPY_INTO_FORCE No <p>Acceptable values - Y/N/NULL</p><p>When set to 'Y', this option will include the FORCE = TRUE option on COPY INTO commands. Data is loaded regardless of whether source file is unchanged or data already exists in target table. Can produce duplicate data in target table </p> ADDITIONAL_FIELDS No Specify any additional columns/fields to be added to generated SELECT clause from source file, that are not available in source. <p>E.g.</p><p>TO_NUMBER(:1) COBID</p><p>would add a column to generated SELECT statement, where it is transforming bind variable value passed in at run time, and aliased as COBID, which would then become part of COPY INTO statement.</p><p> **NOTE: Source file must be a CSV. **"},{"location":"reference/#delete","title":"DELETE","text":"<p>This effectively generates a \"DELETE FROM [target table] additionally WHERE\", if applicable</p> <p>Following are the fields applicable for DELETE command type:</p> Field Name Mandatory? Description CMD_TGT Yes Specify the name of table from which data is to be deleted. <p>Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_WHERE No This is where you can specify a filter clause that gets added as a WHERE clause at run time. <p>WHERE keyword should not be included and numbered bind variables can be used for which actual bind replacements are mentioned in CMD_BINDS E.g.<p>\"C_MKTSEGMENT = :2 AND COBID = :1\"</p>In the above example values for bind variables are passed at run time, and derivation of value for bind variable according to the sequence is derived from CMD_BINDS CMD_BINDS No If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpreted as keys from the variable values JSON object passed in at run-time</p>"},{"location":"reference/#dq_test","title":"DQ_TEST","text":"<p>This command type is for specifying Data Quality Tests within data pipeline. All active Data Quality Tests defined on the target that is specified here, are run within this step.</p> <p>Following are the fields applicable for DQ_TEST command type:</p> Field Name Mandatory? Description CMD_TGT Yes Specify the name of table/view on which data quality tests are to be run from which data is to be deleted.<p>This can also accept pipe delimited multiple targets, which gives a flexibility to define running of DQ Tests on multiple targets in a single step.</p><p>Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_WHERE No This is where you can specify a filter clause that gets added as a WHERE clause at run time. <p>WHERE keyword should not be included and numbered bind variables can be used for which actual bind replacements are mentioned in CMD_BINDS E.g.<p>\"C_MKTSEGMENT = :2 AND COBID = :1\"</p>In the above example values for bind variables are passed at run time, and derivation of value for bind variable according to the sequence is derived from CMD_BINDS CMD_BINDS No If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpreted as keys from the variable values JSON object passed in at run-time</p>"},{"location":"reference/#merge","title":"MERGE","text":"<p>This command type is where we want to use a MERGE statement. It supports either WHEN MATCHED or WHEN NOT MATCHED or both.</p> <p>Following are the fields applicable for MERGE command type:</p> Field Name Mandatory? Description CMD_SRC Yes Specify the name of source of data here. <p>This is usually a data table or a view that encapculates the transformation business logic. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_TGT Yes Specify the name of target destination of data here. <p>This is usually a table. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_WHERE No This is where you can specify a filter clause that gets added to source as a WHERE clause at run time. <p>WHERE keyword should not be included and numbered bind variables can be used for which actual bind replacements are mentioned in CMD_BINDS E.g.<p>\"C_MKTSEGMENT = :2 AND COBID = :1\"</p>In the above example values for bind variables are passed at run time, and derivation of value for bind variable according to the sequence is derived from CMD_BINDS CMD_BINDS No If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpreted as keys from the variable values JSON object passed in at run-time</p> MERGE_ON_FIELDS Yes Here you specify columns that are to be used in generated MERGE SQL in the ON join clause. Multiple fields delimited by Pipe \"|\" symbol GENERATE_MERGE_MATCHED_CLAUSE No Acceptable values are Y/N. When set to Y, \"WHEN MATCHED UPDATE\" clause is added to generated MERGE statement<p>Either this field or GENERATE_MERGE_NON_MATCHED_CLAUSE should be set to Y GENERATE_MERGE_NON_MATCHED_CLAUSE No Acceptable values are Y/N. When set to Y, \"WHEN NOT MATCHED INSERT\" clause is added to generated MERGE statement<p>Either this field or GENERATE_MERGE_MATCHED_CLAUSE should be set to Y ADDITIONAL_FIELDS No This is where you specify any additional columns/fields to be added to generated SELECT clause from source, which is not available in source. <p>E.g.</p><p>TO_NUMBER(:1) COBID</p><p>would add a column to generated SELECT statement, where it is transforming bind variable value passed in at run time, and aliased as COBID, which would then become part of INSERT/MERGE statement</p> TEMP_TABLE No Acceptable values - Y/N/NULL <p>When set to Y, this would trigger creating a temporary table with the same name as target in the same schema as target before the operation of the step is run.</p><p>This feature utilises special functionality that has been introduced in Snowflake, that you can have permanent (or transient) table and a temporary table with the same name and temporary table is then given priority in the current running session.</p><p>In TiPS we utilise this functionality where a data pipeline can be run concurrently withing multiple sessions with its own bind variables and dataset are consistently transformed and published at session level</p>"},{"location":"reference/#publish_scd2_dim","title":"PUBLISH_SCD2_DIM","text":"<p>This command type is specifically created for populating data to SCD (Slowly Changing Dimension) Type 2, where updates to attributes of dimension are handled by creating a version of record with latest values and closing off previous version. This is done by setting appropriate values to \"EFFECTIVE_START_DATE\", \"EFFECTIVE_END_DATE\" and \"IS_CURRENT_ROW\" columns.</p> <p>Following are the fields applicable for PUBLISH_SCD2_DIM command type:</p> Field Name Mandatory? Description CMD_SRC Yes Specify the name of source of data here. <p>This is usually a data table or a view that encapsulates the transformation business logic. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_TGT Yes Specify the name of target destination of data here. <p>This is usually a table. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> BUSINESS_KEY Yes It is column(s) delimitted by Pipe \"|\" symbol that define a business key (also referred as natural key) for a dimension table. <p>For a slowly changing dimension, this is combination of key columns that uniquely identifies a row in the dimension (not a surrogate key), excluding record effective dates and/or current record flag CMD_WHERE No This is where you can specify a filter clause that gets added to source as a WHERE clause at run time. <p>WHERE keyword should not be included and numbered bind variables can be used for which actual bind replacements are mentioned in CMD_BINDS E.g.<p>\"C_MKTSEGMENT = :2 AND COBID = :1\"</p>In the above example values for bind variables are passed at run time, and derivation of value for bind variable according to the sequence is derived from CMD_BINDS CMD_BINDS No If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpretted as keys from the variable values JSON object passed in at run-time</p> ADDITIONAL_FIELDS No This is where you specify any additional columns/fields to be added to generated SELECT clause from source, which is not available in source. <p>E.g.</p><p>TO_NUMBER(:1) COBID</p><p>would add a column to generated SELECT statement, where it is transforming bind variable value passed in at run time, and aliased as COBID, which would then become part of INSERT/MERGE statement</p> TEMP_TABLE No Acceptable values - Y/N/NULL <p>When set to Y, this would trigger creating a temporary table with the same name as target in the same schema as target, before the operation of the step is run.</p><p>This feature utilises special functionality that has been introduced in Snowflake, that you can have permanent(or transient) table and a temporary table with the same name and temporary table is then given priority in the current running session.</p><p>In TiPS we utilise this functionality where a data pipeline can be run concurrently withing multiple sessions with its own bind variables and dataset are consistently transformed and published at session level</p>"},{"location":"reference/#refresh","title":"REFRESH","text":"<p>This command type is for running a DELETE/TRUNCATE SQL command on target and then consecutively running INSERT SQL command. REFRESH command type supports \"DELETE then INSERT\", \"OVERWRITE INSERT\" or \"TRUNCATE then INSERT\", one of which should be specified with \"REFRESH_TYPE\" field setting.</p> <p>Following are the fields applicable for REFRESH command type:</p> Field Name Mandatory? Description CMD_SRC Yes Specify the name of source of data here. <p>This is usually a data table or a view that encapsulates the transformation business logic. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> CMD_TGT Yes Specify the name of target destination of data here. <p>This is usually a table. Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p> REFRESH_TYPE Yes Acceptable values are DI/TI/OI <p>DI (Delete Insert) - Before inserting the data in target, a delete command is run (where optionally filter clause can be added through CMD_WHERE).</p><p>TI (Truncate Insert) - Before inserting the data in target, truncate command on target is run.</p><p>OI (Overwrite Insert) - Before inserting the data in target, any existing data is removed from target. This works similar to truncate, with the caveat that TRUNCATE is a DDL command invoking a commit to the transaction where OVERWRITE doesn't commits the transaction immediately after delete, thus tables is rolled back to previous state if INSERT DML throws an error. CMD_WHERE No This is where you can specify a filter clause that gets added to source as a WHERE clause at run time. <p>WHERE keyword should not be included and numbered bind variables can be used for which actual bind replacements are mentioned in CMD_BINDS E.g.<p>\"C_MKTSEGMENT = :2 AND COBID = :1\"</p>In the above example values for bind variables are passed at run time, and derivation of value for bind variable according to the sequence is derived from CMD_BINDS CMD_BINDS No If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpreted as keys from the variable values JSON object passed in at run-time</p> ADDITIONAL_FIELDS No This is where you specify any additional columns/fields to be added to generated SELECT clause from source, which is not available in source. <p>E.g.</p><p>TO_NUMBER(:1) COBID</p><p>would add a column to generated SELECT statement, where it is transforming bind variable value passed in at run time, and aliased as COBID, which would then become part of INSERT/MERGE statement</p> TEMP_TABLE No Acceptable values - Y/N/NULL <p>When set to Y, this would trigger creating a temporary table with the same name as target in the same schema as target before the operation of the step is run.</p><p>This feature utilises special functionality that has been introduced in Snowflake, that you can have permanent (or transient) table and a temporary table with the same name and temporary table is then given priority in the current running session.</p><p>In TiPS we utilise this functionality where a data pipeline can be run concurrently withing multiple sessions with its own bind variables and dataset are consistently transformed and published at session level</p>"},{"location":"reference/#truncate","title":"TRUNCATE","text":"<p>This command type is for running TRUNCATE SQL command on the defined target.</p> <p>Following are the fields applicable for TRUNCATE command type:</p> Field Name Mandatory? Description CMD_TGT Yes Specify the name of table from which data is to be deleted. <p>Please include schema name with the object name e.g. [SCHEMA NAME].[OBJECT NAME] and all in CAPS please</p>"},{"location":"reference/#proc","title":"PROC","text":"<p>This command type is for running 'call ()' commands. <p>The following fields are applicable for the PROC command type:</p> Field Name Mandatory? Description CMD_SRC Yes Specify the name of the procedure here, referencing cmd_binds in the arguments if required. <p> e.g. SCHEMA1.PROC1(PARAM1 =&gt; \\':1\\', PARAM2 =&gt; :2)  Please include schema name with the procedure name e.g. [SCHEMA NAME].[PROCEDURE NAME] and all in CAPS please</p> CMD_BINDS No If bind variables are used in the step, here you specify the name for bind variables, delimited by Pipe \"|\" symbol. In this case, CMD_BINDS can be passed to the procedure if referenced in CMD_SRC.  <p>Bind variable values are passed in at runtime in a JSON format. Names of bind variables defined here are interpreted as keys from the variable values JSON object passed in at run-time</p>"},{"location":"tips_conventions/","title":"TiPS Conventions","text":""},{"location":"tips_conventions/#must-know","title":"Must Know","text":""},{"location":"tips_conventions/#columns-in-generated-dmls","title":"Columns in Generated DMLs","text":"<p>TiPS automatically identifies common columns between Source and Target, that are then used in generated DMLs. Hence, columns that are needed to be part of generated DMLs in TiPS, should be present in both source and target and with the same name. In select statement of views, aliases can be used where applicable.</p>"},{"location":"tips_conventions/#must-follow","title":"Must Follow","text":""},{"location":"tips_conventions/#scd-slow-changing-dimension-type-2","title":"SCD (Slow Changing Dimension) Type 2","text":"<p>When using PUBLISH_SCD2_DIM command type for populating SCD Type2, please make sure you following columns are included in Dimension Table</p> <ul> <li>EFFECTIVE_START_DATE with datatype DATE, can additionally have NOT NULL constraint.</li> <li>EFFECTIVE_END_DATE with datatype DATE, can additionally have NOT NULL constraint.</li> <li>IS_CURRENT_ROW with datatype BOOLEAN, can additionally have NOT NULL constraint.</li> <li>BINARY_CHECK_SUM with datatype NUMBER. This is a virtual column that contains hashed value of columns that are to be checked for changed values. <p>E.g.</p><p><code>BINARY_CHECK_SUM NUMBER (38,0) AS (HASH(CUSTOMER_NAME, CUSTOMER_ADDRESS, CUSTOMER_PHONE, CUSTOMER_MARKET_SEGMENT, CUSTOMER_COMMENT, COUNTRY, REGION))</code> <p>View/Table that is the source for Dimension, should only have EFFECTIVE_START_DATE, and this should be populated with the date that the new record should be effective from. </p>"},{"location":"tips_conventions/#merge-keyid-column-to-be-populated-from-sequence","title":"MERGE - Key/ID Column to be populated from Sequence","text":"<p>Where a target table needs to have Key/ID column that has to be populated from a sequence, please make sure that column name ends with <code>_KEY</code> or <code>_SEQ</code>. And also make sure that SEQUENCE name starts with <code>SEQ_</code> followed by target table name and is created in same schema as that of target table.</p>"},{"location":"tips_conventions/#schema-names-prepended-to-objects","title":"Schema Names prepended to Objects","text":"<p>In <code>PROCESS_CMD</code> metadata table, wherever database objects are referenced, please make sure to include the schema name E.g. <code>[SCHEMA NAME].[TABLE NAME]</code></p>"},{"location":"tips_conventions/#upper-case-for-db-objects","title":"Upper Case for DB Objects","text":"<p>In <code>PROCESS_CMD</code> metadata table, wherever database objects are referenced, please make sure to enter object names (including schema name), all in UPPERCASE.</p>"},{"location":"tips_conventions/#no-white-spaces-in-process-name","title":"No White Spaces in Process Name","text":"<p>In <code>PROCESS</code> table, or wherever process name is referenced, don't include any white spaces in the process name. Instead, underscore can be used to indicate a logical split between words.</p>"},{"location":"tips_conventions/#good-to-follow","title":"Good to Follow","text":"<ul> <li><code>NOT NULL</code> constraint should be used, where possible.</li> <li>It is a good practise, not to allow NULL values in tables in presentation layer (accessible by end user). If NULL values are likely to be encountered from source systems, a sensible replacement of null values should be used as a standard.</li> <li>For text fields, standards around casing (uppercase or lowercase or any other as preferred) should be adopted when data is populated in presentation layer tables (or tables accessed by end user).</li> </ul>"}]}